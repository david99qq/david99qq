
import os
import numpy as np
import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
import pandas_datareader.data as web
import datetime as dt
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from xgboost import plot_importance, plot_tree
from matplotlib import dates as mdates
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, silhouette_samples, silhouette_score

import plotly as py
import plotly.io as pio
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot

df= web.DataReader('AAPL', 'yahoo',start='2016-8-1',end= '2018-12-1')
df['EMA_9'] = df['Close'].ewm(9).mean().shift()
df['SMA_5'] = df['Close'].rolling(5).mean().shift()
df['SMA_10'] = df['Close'].rolling(10).mean().shift()
df['SMA_15'] = df['Close'].rolling(15).mean().shift()
df['SMA_30'] = df['Close'].rolling(30).mean().shift()
EMA_12 = pd.Series(df['Close'].ewm(span=12, min_periods=12).mean())
EMA_26 = pd.Series(df['Close'].ewm(span=26, min_periods=26).mean())
df['MACD'] = pd.Series(EMA_12 - EMA_26)
df['MACD_signal'] = pd.Series(df.MACD.ewm(span=9, min_periods=9).mean())

df_new=df[[ 'Volume','EMA_9','SMA_5','SMA_10','SMA_15','SMA_30','MACD','MACD_signal','Adj Close']]
df_new['Adj Close'] = df_new['Adj Close'].shift(-1)
data_new=df_new.dropna()
data_new.info()

#X,y split 
X,y = df_new.iloc[:,:-1],df_new.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#model
model = xgb.XGBRegressor(seed=123,n_estimators=10 )
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(f'y_true = {np.array(y_test)[:5]}')
print(f'y_pred = {y_pred[:5]}')

#hypertuning
parameters = {
    'n_estimators': [100, 200, 300,400],
    'learning_rate': [0.001, 0.005, 0.01,0.005],
    'max_depth': [8, 10, 12,14],
    'gamma': [0.001, 0.005, 0.01, 0.02],
    'random_state': [42]
}


model = xgb.XGBRegressor( objective='reg:squarederror')
clf = GridSearchCV(param_grid=parameters, estimator=model)

clf.fit(X_train, y_train)

print(f'Best params: {clf.best_params_}')
print(f'Best validation score = {clf.best_score_}')
model = xgb.XGBRegressor(**clf.best_params_)
model.fit(X_train, y_train,  verbose=False)

#pred
y_pred = model.predict(X_test)
print(f'y_true = {np.array(y_test)[:5]}')
print(f'y_pred = {y_pred[:5]}')

#rmse
y_test=y_test.dropna()
y_pred=y_pred[0:176]
rmse = np.sqrt(mean_squared_error( y_test,y_pred))
print("RMSE: %f" % (rmse))

#score
print("Best score is {}".format(clf.best_score_))



